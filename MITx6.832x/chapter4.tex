\chapter{Lecture 5/6: Acrobots, Cart-poles, and Quadrotors}
\section{Introduction}
So far we have covered the following topics:
\begin{itemize}
\item Manipulator Equations
\item Feedback Linearization
\item Optimal Control
\item Value Iteration (Algorithm for DDP in discrete time)
\end{itemize}
After introducing basics of "classic" non-linear control, we started thinking about control as optimization.

In this chapter the most simple standard models for underactuated robots are introduced. These low-dimensional systems are supposed to capture the essence of the problem without all the real-world complexity of advanced systems. 


\section{Modeling of the Systems}
The Acrobot is a simple underactuated system since it has two DoF. But, in comparison to the double pendulum, it only has one actuator at the elbow so that 
$\myM{B}=\begin{bmatrix} 0 & 1 \end{bmatrix}^T $.
Manipulator Equations: 
\begin{equation*}
\myM{M}(\myM{q})\myM{\ddot{q}}+\myM{C}(\myM{q,\dot{q}})\myM{\dot{q}}=\tau_g(\myM{q})+\myM{Bu}.
\end{equation*}
The goal is to swing-up and balance while satisfying some torque limits. One possible approach to solve this problem is using \textit{value iteration}. But the grids would have to be very fine, in order to get a good solution. There are better tools to solve this problem: LQR!


\section{LQR Control}
\subsection{Recap LQR}
We have a linear time-invariant system in state-space form
\begin{equation*} 
\myM{\dot{x}}=\myM{Ax}+\myM{Bu},
\end{equation*}
the cost function is 
\begin{equation*} 
J = \int_0^{\infty}[\myM{x}^T\myM{Qx}+\myM{u}	^T\myM{Ru}]dt,
\end{equation*}
and the goal is to find the optimal cost-to-go function $J^\star(\myM{x})$ which satisfies the Hamilton-Jacobi-Bellman equation.
This yields 
\begin{equation*} 
J^\star(\myM{x}) = \myM{x}^T\myM{Sx}
\end{equation*}
and the optimal control policy 
\begin{equation*} 
\myM{u}^\star=\myM{Kx}.
\end{equation*}
In the end, this means you get the control policy and the cost-to-go function by
\begin{equation*} 
\myM{K,S}=LinearQuadraticRegulator(\myM{A}, \myM{B}, \myM{Q}, \myM{R}).
\end{equation*}
So you set $\myM{A}, \myM{B}$ from linearization and choose $\myM{Q}, \myM{R}$ and receive an optimal controller.

\subsection{Linearization of Nonlinear Systems}
\textbf{Problem:} Our systems are non-linear! How shall we apply Linear-Quadratic Control?!

\textbf{Solution:} We linearize our system around a specific fixed point.

But we need to be aware that our linearization only is valid within a certain area around this point. If you go to far away from it, the non-linearity overwhelms your solution. 

One Optimal Control Algorithm therefore is to combine multiple LQR Controllers that treat all relevant fixed points in order to handle the relevant workspace. 

\section{Throwback: Hand-Designed Control}
\begin{itemize}
\item Optimal control is a powerful framework for solving control problems via optimization.
\item Solving OC problems for non-linear systems is hard!
\item Sometimes we would be happy to just have any controller (And sometimes they turn out to be even better).
\item But how can we proof this "hand-designed" are any good? 

-> We proof that they are stable. 
\end{itemize}

\subsection{Stability Lyapunov Analysis + Control Design}
We are going to observe the total Energy of the system for analyzing the stability. Lyapunov functions generalize this notion of an energy function to more general systems, which might not be stable in the sense of some mechanical energy. If I can find any positive function, call it $V(\myM{x})$, that gets smaller over time as the system evolves, then I can potentially use $V$ to make a statement about the long-term behavior of the system. $V$ is called a Lyapunov function. 


\subsection{Energy Shaping}
\subsection{Partial Feedback Linearisation}

























