\chapter{Lecture 10: Trajectory Optimization}
\section{Recap: What Did We Cover so Far?}
The\textbf{ overall goal} is to specify complex behaviors with simple objective functions, letting the dynamics and constraints on the system shape the resulting feedback controller.

But the computational tools that we've provided so far have been \textbf{limited} in some important ways:
\begin{itemize}
\item \textbf{Dynamic programming} involves putting a mesh over the state space -> Stuck in low-dimensional systems.
\item \textbf{LQR} + \textbf{Linearization} around an operating point; applicable to high-dimensional systems -> Linearization only valid for a certain region of the state space
\item \textbf{Lyapunov Analysis via SDP/SOS} softens the requirements by not searching for the optimal controller, but only searching for stability. It does so for non-linear systems and all $\myM{x}$ (whole state-space)-> No controller actually synthesised + somehow limited to simple Lyapunov functions (quadratic, polynomials etc) which might limit the control design  
\end{itemize}
But we have not yet provided any real computational tools for approximate optimal control that work for high-dimensional systems beyond the linearization around a goal. That is precisely the goal for this chapter.

\textbf{Q:} How can we handle complex systems?

\textbf{A:} We ask not for all states (like in Lyapunov Analysis), instead only the relevant ones!
\begin{itemize}
\item For the beginning: Consider only one single initial condition!
\item Maybe some $x_0$ in the neighbourhood are also good
\item Represent the solution as a trajectory,  $\myM{x}(t), \myM{u}(t)$, typically defined over a finite interval (instaed of feedback control function)
\item \textbf{This means:} 
\begin{enumerate}
\item We define only a single trajectory $\myM{u}(t)$, 
\item Search for optimal states $\myM{x}$ and control inputs $\myM{u}$,
\item To follow this trajectory as good as possible!
\end{enumerate}
\end{itemize}


\section{Problem Formulation}
\begin{itemize}
\item Min over a finite trajectory over time $\myM{u}(\cdot)$
\item Initial conditions $x(0)$ are fixed and known
\end{itemize}
\begin{align*}
\min_{\myM{u}(\cdot)} \quad &
\int_{t_0}^{t_f} \ell(\myM{b}(t),\myM{b}(t)) dt \\ \text{subject to} \quad &
\forall t, \dot{\myM{x}}(t)=f(\myM{x}(t),\myM{u}(t)), \\
& \myM{x}(t_0)=x_0
\end{align*}


\section{Computational Tools for Nonlinear Optimization}
Intro 
\section{Trajectory Optimization as a Nonlinear Program}
As written above, the optimization above is an optimization over continuous trajectories. In order to formulate this as a numerical optimization, we must parameterize it with a finite set of numbers. 

In order to numerically solve this problem, we need to \textbf{discretise} this (continuous) formulation \textbf{over time}. Different approaches on how to do this, are:
\begin{itemize}
\item Idea 1). \textbf{Direct Transcription}: Fix breakpoints at even intervals $dt$ and use Euler integration. Decision variables are $x,n$ 
\item Idea 2.) \textbf{Direct Shooting Methods} - Idea: Destrict decision variables to only $u$ and and compute $x$ ourselves by knowing $x_0$ and $u(\cdot)$ via the simple forward dynamics
$$x[t=0] = x_0$$
$$x[1]=Ax_0+Bu[0]$$
$$x[2]=A\cdot(x[1]=Ax_0+Bu[0])+Bu[0]$$
$$...$$
\item Idea 3.) \textbf{Direct Collocation:} Assume first-order polynomial for $\myM{u}(t)$ and cubic polynomial $\myM{x}(t)$.
\end{itemize}
\section{Pontryagins's Minimum Principle}
