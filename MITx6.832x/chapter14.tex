\chapter{Lecture 22/23: Reinforcement Learning}
Reinforcement learning (RL) is a \textbf{collection of algorithms} for solving the same optimal control problem that we've focused on through the text, but the real gems from the RL literature are the algorithms for (almost) \textbf{black-box optimization} of \textbf{stochastic optimal control problems}. The idea of an algorithm that only has a "black-box" interface to the optimization problem means that it can obtain (potentially noisy) samples of the optimal cost via trial and error, but does not have access to the underlying model, and does not have direct access to complete gradient information.

\begin{itemize}
\item How do we optimize? "Black-box Optimization": $\sum g(x,u)$ -> We \textit{don't} have a model, only costs
\item What do we optimize? "Stochastic OC Problems": $\min \myM{E}[(x,u)]$ with \textit{random} variables
\end{itemize}

Motivation for RL:
\begin{itemize}
\item For complex problems: When we don't have a model, or the model would be to complex for control design
\end{itemize}

Relation RL / OC:
\begin{itemize}
\item RL more general 
\item OC more efficient (RL random try/error)
\end{itemize}

Drawbacks RL
\begin{itemize}
\item Computationally extremely consumptive (try and error)
\item Manual tuning of parameters necessary
\end{itemize}

\textbf{Overall objective to define}: What to learn? 
\begin{enumerate}
\item Learn $f(x,u)$: System identification + Model-based control
\item Learn $\Pi(x)$: Policy Search -> Section \ref{sec:policy-based}
\item Learn $\hat{J}(x)$: Cost-to-go -> Section \ref{sec:value-based}
\end{enumerate}

\section{Policy-Based Methods}\label{sec:policy-based}
\subsection{The Policy Gradient "Trick"}
One of the standard approaches to policy search in RL is to estimate the gradient of the expected long-term cost with respect to the policy parameters by evaluating some number of sample trajectories, then performing (stochastic) gradient descent. Many of these so-called "policy gradient" algorithms leverage a derivation called the likelihood ratio method that was perhaps first described in [2] then popularized in the the REINFORCE algorithm [3]. It is based on what looks like a trick with logarithms to estimate the gradient

The surprise: I can find the gradient of the long-term cost by taking only the gradient of the policy... but not the gradient of the plant, nor the cost! The intuition is that one can obtain the gradient by evaluating the policy along a number of (random) trajectory roll-outs of the closed-loop system, evaluating the cost on each, then increasing the probability in the policy of taking the actions correlated with lower long-term costs.


\section{Sample Performance via the Signal-to-Noise Ratio}
The simplicity of the REINFORCE / weight perturbation updates makes it tempting to apply them to problems of arbitrary complexity. But a major concern for the algorithm is its performance - although we have shown that the update is in the direction of the true gradient on average, it may still require a prohibitive number of computations to obtain a local minima.
In this section, we will investigate the performance of the weight perturbation algorithm by investigating its signal-to-noise ratio (SNR).


\section{Value-Based Methods}\label{sec:value-based}
Key Idea: Learning value function.
