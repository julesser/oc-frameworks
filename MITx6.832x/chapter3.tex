\chapter{Lecture 3/4: Dynamic Programming}\label{lecture3}
\section{Control as Optimization}
\begin{itemize}
\item The big idea is to formulate control design as an optimization problem.
\item Given a trajectory $x(.),u(.)$ we want to assign a score (scalar) to decribe the performance.
\item Additionally we can set constraints in order to exclude trajectories that exceed certain limits (e.g. control limit $\abs{u(t)}<=1$).
\item The goal is to find a control policy $u=\Pi(t,x)$ that optimizes that score!
\item When solving optimal control problems, one most often needs numerical approximation.
\end{itemize}
The strengths of Optimal Control are that it
\begin{itemize}
\item is a very general approach; it can be applied to fully/underactuated, linear/nonlinear systems,
\item contains an very intuitive approach by describing just the goal and some constraints 
\item works very well with numerical approximation.

\end{itemize}
 
 
\section{Example: Double Integrator}\begin{itemize}
\item Very simple example that can be solved without numerical approximation.
\item Consists of "brick of ice" on a flat floor
\item Goal: Go to origin as fast as possible
\end{itemize}
Easiest case: Formulate optimal control as "Bang-Bang". Accelerate (full throttle) then slam on the brakes.


\section{DDP: Discrete Time Space - Optimal Control as Graph Search}
For systems with a finite, discrete set of states and a finite, discrete set of actions, dynamic programming also represents a set of very efficient numerical algorithms which can compute optimal feedback controllers.

Cost function
$$one-step cost: g(s,a)$$
$$total cost: \sum_{n=0}^{\infty} $$
Key idea: Additive cost 
$$\int_0^T \ell(x(t),u(t)) dt,$$
There are existing numerous possibilities on how to design the cost function:
\begin{itemize}
\item Min-time: $g(s,a) = 1 "if" s~=s_{goal}; 0 if otherwise$
\item Quadratic cost: $g(x,u)=x^{T}x+u^{T}u$
\end{itemize}
There are many algorithms for finding (or approximating) the optimal path from a start to a goal on directed graphs. In dynamic programming, the key insight is that we can find the shortest path from every node by solving recursively for the optimal cost-to-go (the cost that will be accumulated when running the optimal controller) from every node to the goal.
Recursive form of the optimal control problem:
\begin{equation} \hat{J}^*(s_i) \Leftarrow \min_{a \in A} \left[ \ell(s_i,a) +
    \hat{J}^*\left({f(s_i,a)}\right) \right]
    \end{equation}
If we know the optimal cost-to-go, then it's easy to extract the optimal policy:
\begin{equation} \pi^{*}(s_i) = argmin_a
    \left[ \ell(s_i,a) + J^*\left( f(s_i,a) \right) \right].
\end{equation}

Limitations:
\begin{itemize}
\item Accuracy for continuous systems (discretisation error)
\item Scaling (curse of dimensionality)
\item Assumes full state information: Absolutely not necessary to know everything
\end{itemize}


\section{DDP: Continuous Time Space}
\subsection{The Hamilton-Jacobi-Bellman Equation}
An analogous set of conditions can be found in the continuous time space. For a system
$$ \dot{x}=f(x,u)$$
and an infinite-horizon additive cost
$$\int_0^\infty l(x,u)dt $$
we have
$$0=min_u \begin{bmatrix}
l(x,u)+\dfrac{\delta J^*}{\delta x}f(x,u)
\end{bmatrix}$$
$$\Pi^\star=argmin_u\begin{bmatrix}
l(x,u)+\dfrac{\delta J^*}{\delta x}f(x,u)
\end{bmatrix}$$







